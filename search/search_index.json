{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"This is my title","text":"<p>And this is some text</p>"},{"location":"Compute%20System%20Information/compute-resources/","title":"Compute Resources","text":""},{"location":"Policies/usage-policy/","title":"Usage policy","text":"<p>To ensure the stability and usability of the platform for all users, there are certain rules and policies to abide by when using MARS. This is a living document, so changes can be made at any time. We will inform of major changes through our Team (Microsoft Teams). If you have any questions regarding the contents of this page, feel free to get in contact with us.</p> <ol> <li>Jobs must be run using the Slurm queueing system, interactive or batch jobs. Logging into any worker-nodes directly is forbidden.</li> <li>No compute intensive task is to be run on the login-node (mars-login.ice.gla.ac.uk). The login-node is to be used for minor code/script editing, data management, submitting and monitoring jobs. The MARS team reserves the right to kill processes, that constrain the login-node, without warning.</li> <li>Job submissions must specify the resources to the best estimate of the user, as to not waste cluster resources. This ensures the queueing system runs optimally and therefore the user\u2019s jobs are 5. starting as soon as possible. The MARS team reserves the right delete poorly specified jobs.</li> <li>Any data left on scratch spaces after the runtime of the job, may be deleted at any time and without notice.</li> <li>All storage on MARS is to be used only for active processing, and not for long term storage purposes. Please ensure, that all your scripts, datasets and results are saved to another system, as the MARS team offers limited data recovery services.</li> <li>Access to Project shares will only be granted with permission from the PI.</li> <li>The MARS team reserves the right to delete, kill or hold jobs or processes that have an adverse effect on the whole system\u2019s stability or health.</li> <li>The job submission system has the capability to send automated emails about the progress of jobs. Users should ensure that the number of requested emails is not excessive and refrain from sending emails to any email address outside the University of Glasgow domain. This means that affiliate/external user accounts should be using a UoG email address for job alerting.</li> <li>MARS is to be used for research purposes. Teaching and courses are not to be held using this cluster unless it was agreed upon prior with the MARS team.</li> <li>There is a maximum number of resources a single user can use on MARS. If their work needs more than the default, a project should be requested, as described here. The MARS team reserves the right to decline a project application, for reasons provided.</li> <li>To ensure the stability and security of MARS, maintenance is needed. This might result in access being restricted to the cluster or parts of it for certain periods of time. These maintenance windows will be announced via our Team (Microsoft Teams).</li> <li>The resources on MARS are free to use by members of the University of Glasgow. We reserve the right to change this and implement a charging model in the future.</li> </ol> <p>Please note that in addition to the MARS Usage Policy users must also comply with the general IT Policy of the University of Glasgow.</p>"},{"location":"user-docs/","title":"User Documentation","text":""},{"location":"user-docs/account-registration/","title":"Getting an Account","text":""},{"location":"user-docs/connecting/","title":"Connecting to Cluster","text":""},{"location":"user-docs/data-management/","title":"Data Management","text":""},{"location":"user-docs/getting-started/","title":"Getting started on Central","text":"<p>You requested your account and are ready to start using the Central HPC! This page shows you to the most important information you need to start your HPC journey.</p>"},{"location":"user-docs/getting-started/#accessing-the-cluster","title":"Accessing the Cluster","text":"<p>To access the system, you log into a login node. The login node is the central point of access to the cluster for all users. This server is not very powerful and should therefore not be used for computational work. Any computational work should go through a job allocation on the scheduler.</p> <p>To get all the information you need to access MARS see Connecting to MARS.</p> <p>If you are new to UNIX console we recommend going through the UNIX Tutorial for Beginners.</p>"},{"location":"user-docs/getting-started/#managing-data","title":"Managing Data","text":"<p>Where and how to store your data can be crucial for the performance of your job. An overview of all data stores, quotas and how to use them is available in Storage and Data.</p> <p>Help on how to transfer your data can be found here Transfer data.</p> <p>Concerns about your research data? This page should help you make decisions on how to handle data on MARS: Research Data on MARS: What to Know</p>"},{"location":"user-docs/getting-started/#using-software-modules","title":"Using Software Modules","text":"<p>MARS offers a basic software catalogue through \u201cmodules\u201d, that prepare your environment and provide executables. An introductions to modules and how to use them can be found at Modules.</p> <p>A list of all available modules installed on MARS is here Available Modules.</p> <p>We also have Software Manuals for all installed modules available. The manuals offer basic information on the software, helpful manuals and links to get more information!</p> <p>For software outside of the software catalogue, users are responsible for their own installations. The MARS admin team is happy to help though!</p>"},{"location":"user-docs/getting-started/#using-slurm","title":"Using Slurm","text":"<p>To use the HPC to its full potential, you must use Slurm, to run/schedule your jobs. For a first start see Slurm Introduction. You can also find a wide variety of documentation, training and help online!</p>"},{"location":"user-docs/getting-started/#etiquette","title":"Etiquette","text":"<p>An HPC system is a shared resource for researchers and students to work with. To ensure all users have a good experience, there are some expected behaviours.</p> <p>More information and some helpful tips on HPC etiquette can be found here: HPC Etiquette</p>"},{"location":"user-docs/getting-started/#support","title":"Support","text":"<p>Need help getting started on MARS? For any inquiry or support, don\u2019t hesitate to contact us!</p>"},{"location":"user-docs/slurm/","title":"Slurm Scheduler","text":""},{"location":"user-docs/software/","title":"Software","text":""},{"location":"user-docs/Slurm%20Guides/pbs-2-slurm/","title":"PBS to Slurm Cheat Sheet","text":""},{"location":"user-docs/Slurm%20Guides/pbs-2-slurm/#user-commands","title":"User Commands","text":"Purpose PBS Slurm Submit a job via script <code>qsub myScript.sh</code> <code>sbatch myScript.sh</code> Cancel a job <code>qdel &lt;JobID&gt;</code> <code>scancel &lt;JobID&gt;</code> List all running jobs <code>qstat</code> <code>squeue</code> List all your running jobs <code>qstat -u &lt;GUID&gt;</code> <code>squeue -u &lt;GUID&gt;</code> Advanced job information <code>qstat -f &lt;JobID&gt;</code> <code>sacct -j &lt;JobID&gt;</code>"},{"location":"user-docs/Slurm%20Guides/pbs-2-slurm/#environment-variables","title":"Environment Variables","text":"Purpose PBS Slurm Job ID <code>$PBS_JOBID</code> <code>$SLURM_JOBID</code> Submit directory <code>$PBS_O_WORKDIR</code> <code>$SLURM_SUBMIT_DIR</code> Allocated node list <code>$PBS_NODEFILE</code> <code>$SLURM_JOB_NODELIST</code> Current node name <code>-</code> <code>$SLURMD_NODENAME</code> Array job index <code>$PBS_ARRAY_INDEX</code> <code>$SLURM_ARRAY_TASK_ID</code> Number of CPUs <code>$PBS_NUM_PPN * $PBS_NUM_NODES</code> <code>$SLURM_NPROCS</code>"},{"location":"user-docs/Slurm%20Guides/pbs-2-slurm/#job-parameters","title":"Job Parameters","text":"Purpose PBS Slurm Job name <code>#PBS -N myJob</code> <code>#SBATCH --job-name=myJob</code> Wallclock limit <code>#PBS -l walltime=hh:mm:ss</code> <code>#SBATCH --time=dd-hh:mm:ss</code> CPU Time <code>#PBS -l cput=hh:mm:ss</code> <code>-</code> Number of nodes and CPUs <code>#PBS -l nodes=2:ppn=8</code> <code>#SBATCH --nodes=2 --ntasks-per-node=1 --cpus-per-task=8</code> Memory <code>-</code> <code>#SBATCH --mem=8G</code> GPU <code>-</code> <code>#SBATCH --gres=gpu:1</code> Array <code>#PBS -t 1-100</code> <code>#SBATCH --array=1-100</code> Select queue / partition <code>#PBS -q &lt;QueueName&gt;</code> <code>#SBATCH --partition=&lt;PartitionName&gt;</code> Working directory <code>-</code> <code>#SBATCH --workdir=/path/to/file</code> STDOUT file <code>#PBS -o /path/to/file</code> <code>#SBATCH --output=/path/to/file</code> STDERR file <code>#PBS -e /path/to/file</code> <code>#SBATCH --error=/path/to/file</code> Email notifications <code>#PBS -m a\\|b\\|e</code> <code>#SBATCH --mail-type=NONE\\|BEGIN\\|END\\|FAIL\\|ALL</code> Email recipient <code>#PBS -M &lt;Email&gt;</code> <code>#SBATCH --mail-user=&lt;Email&gt;</code>"},{"location":"user-docs/Slurm%20Guides/slurm-monitoring/","title":"Slurm Job Monitoring","text":"<p>Slurm is saving accounting data for every job run on the system. The accounting database contains information about exit codes, resource usage, runtime and more. With this utility you can keep track of your usage of the cluster for currently running or finished jobs.</p> <p>To access this information you can use the sacct command. This is a very powerful command and the full documentation can be found here sacct or you can run man sacct on the system. Here some examples:</p> <p>Show all your jobs from the last 7 days:</p> <pre><code>sacct -u &lt;GUID&gt; -S $(date +%Y-%m-%d -d \"-7 days\") -X\n</code></pre> <p>Resource information for a specific job:</p> <pre><code>sacct -j &lt;JobID&gt; -o JobID,JobName,Partition,Account,AllocNodes,AllocTRES -X -P\n</code></pre> <p>For the -o parameter you can choose all the information listed and described here: sacct \u2013 Job-Accounting-Fields</p> <p>An additional command, which bases its data from the Slurm accounting database is the seff command. This should only be used on completed jobs, or the shown data could be wrong.</p> <p>seff gives you a quick overview of resources of a job including the efficiency</p> CentralMARS <pre><code>seff fake &lt;JobID&gt;\n</code></pre> <pre><code>seff &lt;JobID&gt;\n</code></pre>"},{"location":"user-docs/Software%20Guides/apptainer/","title":"Apptainer","text":""},{"location":"user-docs/Software%20Guides/apptainer/#about","title":"About","text":"<p>Apptainer (formerly Singularity) simplifies the creation and execution of containers, ensuring software components are encapsulated for portability and reproducibility.</p> <p>Containers are images run on a server, using base operating system components from that server. It\u2019s similar to a virtual machine, where you can choose your OS and what packages to install within it, without compromising the host system.</p>"},{"location":"user-docs/Software%20Guides/apptainer/#load-module","title":"Load Module","text":"CentralMARS <pre><code>module load apptainer\n</code></pre> <pre><code>module load apps/apptainer\n</code></pre>"},{"location":"user-docs/Software%20Guides/apptainer/#usage","title":"Usage","text":"<p>For usage information run the following command after loading the module:</p> CentralMARS <pre><code>apptainer fake --help\n</code></pre> <pre><code>apptainer --help\n</code></pre> <p>Below we have some easy manuals to get a first feel for containers, though we would recommend going through the more in depth Quick Start guide Apptainer offers in their documentation. This is linked in the \u201cExternal Links\u201d below.</p>"},{"location":"user-docs/Software%20Guides/apptainer/#run-containers","title":"Run containers","text":"<p>On MARS you can run images you made yourself, or run images provided by colleagues or software developers. Please make sure the images you are using are from trusted sources!</p> <p>You can run a container using the <code>apptainer run</code> command. The <code>&lt;image&gt;</code> part can either be the path to a <code>.sif</code> image you have saved locally or it can be a link to an image stored in a repository online.</p> <pre><code>apptainer run &lt;options&gt; &lt;image&gt;\n</code></pre> <p>As an example we will use the image hosted on the GitHub container registry by the Apptainer developers: docker://ghcr.io/apptainer/lolcow. All you need to do is provide the link and Apptainer will run the %runscript code of the image. In our example this will be a cow, telling you the current time and date:</p> <pre><code>[&lt;GUID&gt;@node1 [mars] ~]$ apptainer run docker://ghcr.io/apptainer/lolcow\n ______________________________\n&lt; Tue Feb 11 14:02:52 GMT 2025 &gt;\n ------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>If you want to run a command outside of the workflow defined in <code>%runscript</code>, you can use the <code>apptainer</code> exec command:</p> <pre><code>apptainer exec &lt;options&gt; &lt;image&gt; &lt;command&gt;\n</code></pre> <p>This will not run the code within <code>%runscript</code> and instead run the code you provide on the command line:</p> <pre><code>[&lt;GUID&gt;@node1 [mars] ~]$ apptainer exec docker://ghcr.io/apptainer/lolcow echo \"I won't do what I was made for!\"\nI won't do what I was made for!\n[&lt;GUID&gt;@node1 [mars] ~]$\n</code></pre> <p>If you want to test and debug, you can run an interactive shell from within a container, using the <code>apptainer shell</code> command. To leave just use exit:</p> <pre><code>apptainer shell &lt;options&gt; &lt;image&gt;\n[&lt;GUID&gt;@node1 [mars] ~]$ apptainer shell docker://ghcr.io/apptainer/lolcow\nApptainer&gt; echo $SHELL\n/bin/bash\nApptainer&gt; echo \"Hello, world!\"\nHello, world!\nApptainer&gt; exit\n[&lt;GUID&gt;@node1 [mars] ~]$\n</code></pre> <p>All three of these commands have similar parameters. If you want to see all available options for the command run <code>man apptainer &lt;run/exec/shell&gt;</code> in your console. Below we explain some options we see as important:</p> Option Template Example Description <code>-B, --bind=[]</code> <code>-B &lt;src&gt;:&lt;dest&gt;</code> <code>-B ~/mydata:/datadir</code> Binds the local directory ~/mydata to /datadir within the container. <code>--nv</code> <code>--nv</code> <code>--nv</code> Enables Nvidia support. Use this option when working on GPU-servers to ensure you have the GPU available within your container."},{"location":"user-docs/Tutorials/slurm-101/","title":"Slurm for Beginners","text":""},{"location":"user-docs/Tutorials/unix-101/","title":"UNIX for Beginners","text":""}]}