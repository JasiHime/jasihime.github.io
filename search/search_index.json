{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"University of Glasgow HPC","text":"<p>Welcome to the University of Glasgow HPC documentation platform. </p> <p>You can use the Next and Previous buttons at the bottom of the page to go through the content in a structured way, or use the left-hand navigation to browse. At the top-right of the page is a search bar, which should help you find the content you are looking for.</p> <p>In the grid below we have quick links to content users are frequently looking for:</p> <ul> <li> <p> Get Access to HPC</p> <p>Get access to HPC to start accelarating your compute work!</p> <p> Getting an Account</p> </li> <li> <p> Start your Work</p> <p>This guide will help both novice and experienced users!</p> <p> Quickstart</p> </li> <li> <p> Need help?</p> <p>Open a HPC Support request through Ivanti, and get in contact with the admins!</p> <p> Support</p> </li> <li> <p> Stay Informed</p> <p>Join our Team on Microsoft Teams to be up to date on any news and find likeminded HPC users!</p> <p> Join HPC Community</p> </li> </ul>"},{"location":"account-registration/","title":"Getting an Account","text":"<p>Any staff, student and affiliate user within the University of Glasgow is eligible to use the Central HPC. As affiliate / honorary you\u2019ll need a University of Glasgow email address, which you can request through the Ivanti help desk portal. </p> <p>The account will be bound to your University of Glasgow GUID, and therefore will have the same credentials and be disabled when your main account does.</p> <p>We expect users to request their own account. A user account request should not be submitted by another person on behalf of that user. If you have a request for a bulk account creation of more than 10 accounts, please open a HPC Support request through Ivanti.</p> <p>Use the University of Glasgow\u2019s self-service portal Ivanti to request your account. Choose Central in the \u201cCluster name\u201d dropdown.</p> <p>User Registration</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This manual will give you all the information you need to use the cluster. You can find more guides in the documentation after this page, which will help you with more specialised issues. Or you can find some step by step tutorials you can follow, if this is easier for you to learn with.</p>"},{"location":"quickstart/#introduction","title":"Introduction","text":"<p>HPC stands for High Performance Computing. Commonly this refers to a cluster of servers with ressources shared by multiple people. To manage resource allocation a scheduler is used that based on your definitions creates an isolated work environment to run code. As the resource is shared and not always used evenly, jobs can be queued for a while before they are run by the scheduler. No instant access to resources is quaranteed.</p>"},{"location":"quickstart/#how-can-hpc-help-you","title":"How can HPC help you?","text":"<ul> <li>Your work has outgrown your personal device's resources.</li> <li>Your work runs for hours or days and prevents you from using your personal device for other work.</li> <li>You do not have the right resources, like GPU, available to you in your personal device.</li> <li>You don't have the resources to aquire an own powerful computer and maintain it.</li> </ul>"},{"location":"quickstart/#architecture","title":"Architecture","text":"<p>HPC can be built in in different configurations. This configuration is very popular and you will see it in many other HPCs</p> <p></p> <p>The HPC is build of these components:</p> <ul> <li>Login Node: This is the system all users interact with. If you want to interact with any of the other components, it has to be through the login node.</li> <li>Scheduler: This is the brain of the cluster. All your scheduling and status requests from the login node go this system. </li> <li>Compute Servers: This symbolises all compute servers you could get allocations from through the scheduler. You should not access the servers directly unless you have a allocation. </li> <li>Shared Storage: Most of the storage you interact with on the cluster is shared between the login node and all available compute servers. Therefore you can manage the data you need within your jobs from the login node.</li> <li>HPC Network: The network is shut off from the Campus network and therefore systems within it, will not be accessible to users outside of the login node.</li> </ul>"},{"location":"quickstart/#how-does-hpc-work","title":"How does HPC work?","text":"<p>You will be submitting \"work packages\" in form of submission scripts, that define your resource requirement, an educated estimation of how long your job will run, and the workflow you want to run. Your work should run fully autonomous, this means no human interaction like GUI or console inputs. You can however, also have interactive sessions through HPC, these are useful for environment preparations, tests and debugging. </p> <p>Your work package will then be queued by the scheduler. The scheduler will decide where your job can run, based on your resource requirements described. Multiple jobs can run on one server. If there is enough resource available your job might run right away, otherwise it will be queued and run at a later time. You should be able to provide contact information, and the scheduler will keep you in the loop if your job has started, finished or failed. </p> <p>Scheduling is a complicated matter, and multiple factors play into the priority of your job, however generally, the smaller your job, the faster it will run, so it pays out to be efficient!</p>"},{"location":"quickstart/#access-the-cluster","title":"Access the Cluster","text":"<p>After you got your account, to access the system, you log into a login node. The login node is the central point of access to the cluster for all users. This server is not very powerful and should therefore not be used for computational work. Any computational work should go through a job allocation on the scheduler.</p>"},{"location":"quickstart/#connection-information","title":"Connection Information","text":"<p>You have to be connected to the Campus network either via LAN, eduroam or VPN.</p> Central <ul> <li>Hostname: <code>headnode04.cent.gla.ac.uk</code></li> <li>Username: University of Glasgow GUID</li> <li>Password: GUID Password</li> </ul>"},{"location":"quickstart/#connecting-via-ssh","title":"Connecting via SSH","text":"<p>You will need to use <code>SSH</code> to connect to the login node and use the HPC. The simplest way to connect is by opening a console and connect using the preinstalled <code>SSH</code>utility of your device(If you are prompted for a password, it will not show up while typing):</p> <pre><code>ssh &lt;username&gt;@&lt;hostaname&gt;\n</code></pre> <p>We would recommend you use a SSH GUI client for regular access to the platform, as it allows you to save sessions, and <code>copy+paste</code> more easily. Example software are PuTTY and MobaXterm, however you can use whatever you prefer. </p>"},{"location":"quickstart/#data-management","title":"Data Management","text":"<p>Data is an important part of HPC. Where and how to store your data is important for efficient usage of the platform. </p> <p>All storage available is to be used for the duration of a your work. It is not expected to provide long term/primary storage. The data will assumed to be transient with only limited protection. As the HPC is not a primary storage solution, we recommend storing all HPC data, you can\u2019t afford to lose in a primary, safe location like a centralised storage system provided by your school or a Team within Microsoft Teams. With each Team, you get 5TB of cloud storage. You can use Rclone to manage your data from MARS.</p> <p>Warning</p> <p>This is not a trusted research environment, therefore all research data must be anonymised prior to transferring it onto the system.</p>"},{"location":"quickstart/#storage-spaces","title":"Storage Spaces","text":"Central"},{"location":"quickstart/#user-home","title":"User Home","text":"Size 100G (quota per user) Path <code>/mnt/home/&lt;GUID&gt;</code> Use Set up your environments and store all the scripts and data you need for your personal use."},{"location":"quickstart/#shared-user-scratch","title":"Shared User Scratch","text":"Size ~180Tb (shared between all cluster users) Path <code>~/sharedscratch</code> or <code>/mnt/scratch/users/&lt;GUID&gt;</code> Use This storage is shared between all nodes. Read and write data that you need during your jobs. Please ensure to clean up your scratch space after you are done processing your job, to make the space available for other users to use!"},{"location":"quickstart/#local-node-scratch","title":"Local Node Scratch","text":"Size 700G Path <code>~/localscratch</code> or <code>/tmp/users/&lt;GUID&gt;</code> Use This storage is local to the node and can\u2019t be accessed outside of it. Read and write here for the best possible storage performance. If you drop files into the localscratch of the login node it won\u2019t be available to you on the compute nodes, so the moving of data has to be part of your workflow /submission script. Please ensure to clean up your scratch space after you are done processing your job, to make the space available for other users to use!"},{"location":"quickstart/#transfer-data","title":"Transfer Data","text":"<p>To transfer data from your local machine (or another system), you can use <code>SSH</code>. You can do this either with the <code>scp</code> command:</p> <pre><code>scp &lt;source file&gt; &lt;guid&gt;@&lt;hostname&gt;:&lt;target file&gt;\n</code></pre> <p>Or you can use a graphical SFTP Client of choice, for example WinSCP. Use the connection details of the login node, mentioned above to connect.</p>"},{"location":"quickstart/#scheduler","title":"Scheduler","text":"<p>The scheduler used is Slurm Workload Manager, developed by SchedMD. Slurm has a very in depth documentation themselves, which could be useful to read through, for a more in depth understanding of how this software works Quick Start User Guide. </p> <p>The information here is the configurations you will need to know to use the University of Glasgow HPC.</p>"},{"location":"quickstart/#resources","title":"Resources","text":"<p>Compute servers, or also called nodes, can carry different resource configurations, to fit different workloads. For example, some servers might offer high amount of CPU, while others offer GPU resource.</p> Central <p>The Central HPC is very heterogeneous, meaning it is comprised of a vast variety of hardware! You can  get an overview of all servers and their available resources by running the command below on  the system:</p> <pre><code>sinfo -o \"%20n %10c %20m %30G\"\n</code></pre> Info <ul> <li>CPUS: Number of CPUs available on the node.</li> <li>MEMORY: Amount of memory / RAM available on the node in MB.</li> <li>GRES: GPU resources available on the node. <code>gpu:&lt;type&gt;:&lt;amount&gt;</code>.</li> </ul>"},{"location":"quickstart/#partitions-queues","title":"Partitions / Queues","text":"<p>Partitions, also known as queues on other scheduling systems, are used to determine which nodes you want your job to run. To see the partition configuration of the HPC you are using run this command:</p> <pre><code>scontrol show partition\n</code></pre> Central Name Description cpu This is the default partition, meaning this is chosen when no partition is specified. It contains all CPU focused servers of the Cluster. gpu This partitions cotains all servers with GPU resources available. You can specify which type with the <code>--gres</code> parameter."},{"location":"quickstart/#software","title":"Software","text":"Central <p>On Central, users are responsible for their own software. Though we offer some tools through modules, that make it easier for users to manage software.</p>"},{"location":"quickstart/#environment-modules","title":"Environment Modules","text":"<p>These are software that is centrally installed by the admin team and can be used across the cluster. The full manual of the software from the developers can be found here.</p> <p>To get a list of all available modules, you can run this command on the cluster:</p> <pre><code>module available\n</code></pre> <p>The most used commands are listed below:</p> Command Description <code>module load</code> Activate module for use in your current session. <code>module unload &lt;name&gt;</code> Deactivate module from your current session. <code>module search &lt;search_term&gt;</code> Search for modules by name or description. <code>module list</code> List all active modules in your current session. <code>module purge</code> Deactivate all modules loaded in your current session."},{"location":"quickstart/#support","title":"Support","text":"<p>The RCaaS HPC Admin team is happy to help where possible, no matter if you are a novice or experienced user. Feel free to get in touch! All our services can be found through Ivanti:</p> <p>Ivanti Help Desk</p>"},{"location":"Policies/usage-policy/","title":"Usage policy","text":"<p>To ensure the stability and usability of the platform for all users, there are certain rules and policies to abide by when using MARS. This is a living document, so changes can be made at any time. We will inform of major changes through our Team (Microsoft Teams). If you have any questions regarding the contents of this page, feel free to get in contact with us.</p> <ol> <li>Jobs must be run using the Slurm queueing system, interactive or batch jobs. Logging into any worker-nodes directly is forbidden.</li> <li>No compute intensive task is to be run on the login-node (mars-login.ice.gla.ac.uk). The login-node is to be used for minor code/script editing, data management, submitting and monitoring jobs. The MARS team reserves the right to kill processes, that constrain the login-node, without warning.</li> <li>Job submissions must specify the resources to the best estimate of the user, as to not waste cluster resources. This ensures the queueing system runs optimally and therefore the user\u2019s jobs are 5. starting as soon as possible. The MARS team reserves the right delete poorly specified jobs.</li> <li>Any data left on scratch spaces after the runtime of the job, may be deleted at any time and without notice.</li> <li>All storage on MARS is to be used only for active processing, and not for long term storage purposes. Please ensure, that all your scripts, datasets and results are saved to another system, as the MARS team offers limited data recovery services.</li> <li>Access to Project shares will only be granted with permission from the PI.</li> <li>The MARS team reserves the right to delete, kill or hold jobs or processes that have an adverse effect on the whole system\u2019s stability or health.</li> <li>The job submission system has the capability to send automated emails about the progress of jobs. Users should ensure that the number of requested emails is not excessive and refrain from sending emails to any email address outside the University of Glasgow domain. This means that affiliate/external user accounts should be using a UoG email address for job alerting.</li> <li>MARS is to be used for research purposes. Teaching and courses are not to be held using this cluster unless it was agreed upon prior with the MARS team.</li> <li>There is a maximum number of resources a single user can use on MARS. If their work needs more than the default, a project should be requested, as described here. The MARS team reserves the right to decline a project application, for reasons provided.</li> <li>To ensure the stability and security of MARS, maintenance is needed. This might result in access being restricted to the cluster or parts of it for certain periods of time. These maintenance windows will be announced via our Team (Microsoft Teams).</li> <li>The resources on MARS are free to use by members of the University of Glasgow. We reserve the right to change this and implement a charging model in the future.</li> </ol> <p>Please note that in addition to the MARS Usage Policy users must also comply with the general IT Policy of the University of Glasgow.</p>"},{"location":"clusters/central/","title":"Central HPC Cluster","text":"<p>Information Services run a High Performance Compute (HPC) cluster called Central, that is available to use by all staff, students and affiliates of the University of Glasgow.</p> <p>The platform and team around it have been supporting researchers in all colleges since 2014. Managed by Research Computing as a Service (RCaaS) and motivated by the needs of the community, Central is here for people with large upscaling needs but also users who are just starting their HPC journey.</p>"},{"location":"clusters/central/#technology","title":"Technology","text":"<p>Central offers the following compute resources:</p> <ul> <li>1020 virtual CPUS</li> <li>8 Nvidia GeForce GTX 1080 GPU</li> <li>2 Nvidia L40S GPUs</li> <li>Up to 16GB RAM per core</li> </ul> <p>All servers of the cluster run on Oracle Linux 9. Oracle Linux is a RHEL based open-source operating system. Compute servers are provisioned with a standardised image on each reboot. This allows for easy resetting of malfunctioning nodes and expansion of new hardware.</p> <p>The cluster's scheduler is the Slurm Workload Manager, developed by SchedMD. This software is crucial for HPC and helps acheive fair usage of all available compute resources.</p> <p>The cluster is located in Saughfield House on the University of Glasgow Campus next to the Library building.</p>"},{"location":"clusters/central/#acknowlegement","title":"Acknowlegement","text":"<p>Where Central is used in the development of research outputs the following attribution should be used:</p> <p>This research utilised the University of Glasgow\u2019s Central HPC, supported by University of Glasgow Research Computing as a Service. (link.to.website)</p>"},{"location":"guides/containers/","title":"Apptainer","text":""},{"location":"guides/file-transfer/","title":"Transfer Data","text":""},{"location":"guides/jupyter/","title":"Jupyter","text":""},{"location":"guides/pbs-2-slurm/","title":"PBS to Slurm Cheat Sheet","text":""},{"location":"guides/pbs-2-slurm/#user-commands","title":"User Commands","text":"Purpose PBS Slurm Submit a job via script <code>qsub myScript.sh</code> <code>sbatch myScript.sh</code> Cancel a job <code>qdel &lt;JobID&gt;</code> <code>scancel &lt;JobID&gt;</code> List all running jobs <code>qstat</code> <code>squeue</code> List all your running jobs <code>qstat -u &lt;GUID&gt;</code> <code>squeue -u &lt;GUID&gt;</code> Advanced job information <code>qstat -f &lt;JobID&gt;</code> <code>sacct -j &lt;JobID&gt;</code>"},{"location":"guides/pbs-2-slurm/#environment-variables","title":"Environment Variables","text":"Purpose PBS Slurm Job ID <code>$PBS_JOBID</code> <code>$SLURM_JOBID</code> Submit directory <code>$PBS_O_WORKDIR</code> <code>$SLURM_SUBMIT_DIR</code> Allocated node list <code>$PBS_NODEFILE</code> <code>$SLURM_JOB_NODELIST</code> Current node name <code>-</code> <code>$SLURMD_NODENAME</code> Array job index <code>$PBS_ARRAY_INDEX</code> <code>$SLURM_ARRAY_TASK_ID</code> Number of CPUs <code>$PBS_NUM_PPN * $PBS_NUM_NODES</code> <code>$SLURM_NPROCS</code>"},{"location":"guides/pbs-2-slurm/#job-parameters","title":"Job Parameters","text":"Purpose PBS Slurm Job name <code>#PBS -N myJob</code> <code>#SBATCH --job-name=myJob</code> Wallclock limit <code>#PBS -l walltime=hh:mm:ss</code> <code>#SBATCH --time=dd-hh:mm:ss</code> CPU Time <code>#PBS -l cput=hh:mm:ss</code> <code>-</code> Number of nodes and CPUs <code>#PBS -l nodes=2:ppn=8</code> <code>#SBATCH --nodes=2 --ntasks-per-node=1 --cpus-per-task=8</code> Memory <code>-</code> <code>#SBATCH --mem=8G</code> GPU <code>-</code> <code>#SBATCH --gres=gpu:1</code> Array <code>#PBS -t 1-100</code> <code>#SBATCH --array=1-100</code> Select queue / partition <code>#PBS -q &lt;QueueName&gt;</code> <code>#SBATCH --partition=&lt;PartitionName&gt;</code> Working directory <code>-</code> <code>#SBATCH --workdir=/path/to/file</code> STDOUT file <code>#PBS -o /path/to/file</code> <code>#SBATCH --output=/path/to/file</code> STDERR file <code>#PBS -e /path/to/file</code> <code>#SBATCH --error=/path/to/file</code> Email notifications <code>#PBS -m a\\|b\\|e</code> <code>#SBATCH --mail-type=NONE\\|BEGIN\\|END\\|FAIL\\|ALL</code> Email recipient <code>#PBS -M &lt;Email&gt;</code> <code>#SBATCH --mail-user=&lt;Email&gt;</code>"},{"location":"guides/slurm/","title":"Slurm","text":"<p>You can use a whole lot of parameters to specify your resource request to the scheduler. We will cover the most common parameters here, but if you want a full overview of all available ones, you can run <code>man sbatch</code> or <code>man srun</code> when logged into the system. </p> Parameter Example Description <code>-J, --job-name=&lt;jobname&gt;</code> <code>--job-name=\"Test-Job-1\"</code> Specify a name for the job. The specified name will appear along with the job id number when querying running jobs on the system. The default is the supplied executable program's name. <code>-N, --nodes=&lt;num&gt;</code> <code>--nodes=2</code> Number of servers you want your job to run on. Use only if your code supports parallel processing. <code>-c, --cpus-per-task=&lt;num&gt;</code> <code>--cpus-per-task=8</code> Request the number of CPUs to be allocated per process. This may be useful if the job is multithreaded and requires more than one CPU per task for optimal performance. <code>--gres=gpu:&lt;type&gt;:&lt;num&gt;</code> <code>--gres=gpu:gtx1080:1</code> Specify the type and number of GPUs for your allocation. <code>--mem=&lt;num&gt;&lt;unit&gt;</code> <code>--mem=16G</code> Specify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [K <code>-p, --partition=&lt;partition_names&gt;</code> <code>--partition=gpu</code> Request a specific partition for the resource allocation. If the job can use more than one partition, specify their names in a comma separate list. If the parameter is not set it defaults to configured default partition. <code>-t, --time=&lt;dd-hh-mm-ss&gt;</code> <code>--time 00-01:00:00</code> Set a limit on the total run time of the job allocation. When the time limit is reached, each task in each job step is sent a kill signal. <code>--mail-user=&lt;email&gt;</code> <code>--mail-user=\"example@example.co.uk\"</code> Email address to send notifications to. Only University of Glasgow managed emails are accepted. <code>--mail-type=&lt;type&gt;</code> <code>--mail-type=\"BEGIN,END,FAIL\"</code> Comma separated list of event types <code>--mail-user</code> gets notified for.  Valid type values are NONE, BEGIN, END, FAIL, RE\u2010QUEUE, ALL"},{"location":"policies/usage-policy/","title":"Usage policy","text":"<p>To ensure the stability and usability of the platform for all users, there are certain rules and policies to abide by when using MARS. This is a living document, so changes can be made at any time. We will inform of major changes through our Team (Microsoft Teams). If you have any questions regarding the contents of this page, feel free to get in contact with us.</p> <ol> <li>Jobs must be run using the Slurm queueing system, interactive or batch jobs. Logging into any worker-nodes directly is forbidden.</li> <li>No compute intensive task is to be run on the login-node (mars-login.ice.gla.ac.uk). The login-node is to be used for minor code/script editing, data management, submitting and monitoring jobs. The MARS team reserves the right to kill processes, that constrain the login-node, without warning.</li> <li>Job submissions must specify the resources to the best estimate of the user, as to not waste cluster resources. This ensures the queueing system runs optimally and therefore the user\u2019s jobs are 5. starting as soon as possible. The MARS team reserves the right delete poorly specified jobs.</li> <li>Any data left on scratch spaces after the runtime of the job, may be deleted at any time and without notice.</li> <li>All storage on MARS is to be used only for active processing, and not for long term storage purposes. Please ensure, that all your scripts, datasets and results are saved to another system, as the MARS team offers limited data recovery services.</li> <li>Access to Project shares will only be granted with permission from the PI.</li> <li>The MARS team reserves the right to delete, kill or hold jobs or processes that have an adverse effect on the whole system\u2019s stability or health.</li> <li>The job submission system has the capability to send automated emails about the progress of jobs. Users should ensure that the number of requested emails is not excessive and refrain from sending emails to any email address outside the University of Glasgow domain. This means that affiliate/external user accounts should be using a UoG email address for job alerting.</li> <li>MARS is to be used for research purposes. Teaching and courses are not to be held using this cluster unless it was agreed upon prior with the MARS team.</li> <li>There is a maximum number of resources a single user can use on MARS. If their work needs more than the default, a project should be requested, as described here. The MARS team reserves the right to decline a project application, for reasons provided.</li> <li>To ensure the stability and security of MARS, maintenance is needed. This might result in access being restricted to the cluster or parts of it for certain periods of time. These maintenance windows will be announced via our Team (Microsoft Teams).</li> <li>The resources on MARS are free to use by members of the University of Glasgow. We reserve the right to change this and implement a charging model in the future.</li> </ol> <p>Please note that in addition to the MARS Usage Policy users must also comply with the general IT Policy of the University of Glasgow.</p>"},{"location":"software-modules/apptainer/","title":"Apptainer","text":""},{"location":"software-modules/apptainer/#about","title":"About","text":"<p>Apptainer (formerly Singularity) simplifies the creation and execution of containers, ensuring software components are encapsulated for portability and reproducibility.</p> <p>Containers are images run on a server, using base operating system components from that server. It\u2019s similar to a virtual machine, where you can choose your OS and what packages to install within it, without compromising the host system.</p>"},{"location":"software-modules/apptainer/#load-module","title":"Load Module","text":"CentralMARS <pre><code>module load apptainer\n</code></pre> <pre><code>module load apps/apptainer\n</code></pre>"},{"location":"software-modules/apptainer/#usage","title":"Usage","text":"<p>For usage information run the following command after loading the module:</p> CentralMARS <pre><code>apptainer fake --help\n</code></pre> <pre><code>apptainer --help\n</code></pre> <p>Below we have some easy manuals to get a first feel for containers, though we would recommend going through the more in depth Quick Start guide Apptainer offers in their documentation. This is linked in the \u201cExternal Links\u201d below.</p>"},{"location":"software-modules/apptainer/#run-containers","title":"Run containers","text":"<p>On MARS you can run images you made yourself, or run images provided by colleagues or software developers. Please make sure the images you are using are from trusted sources!</p> <p>You can run a container using the <code>apptainer run</code> command. The <code>&lt;image&gt;</code> part can either be the path to a <code>.sif</code> image you have saved locally or it can be a link to an image stored in a repository online.</p> <pre><code>apptainer run &lt;options&gt; &lt;image&gt;\n</code></pre> <p>As an example we will use the image hosted on the GitHub container registry by the Apptainer developers: docker://ghcr.io/apptainer/lolcow. All you need to do is provide the link and Apptainer will run the %runscript code of the image. In our example this will be a cow, telling you the current time and date:</p> <pre><code>[&lt;GUID&gt;@node1 [mars] ~]$ apptainer run docker://ghcr.io/apptainer/lolcow\n ______________________________\n&lt; Tue Feb 11 14:02:52 GMT 2025 &gt;\n ------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>If you want to run a command outside of the workflow defined in <code>%runscript</code>, you can use the <code>apptainer</code> exec command:</p> <pre><code>apptainer exec &lt;options&gt; &lt;image&gt; &lt;command&gt;\n</code></pre> <p>This will not run the code within <code>%runscript</code> and instead run the code you provide on the command line:</p> <pre><code>[&lt;GUID&gt;@node1 [mars] ~]$ apptainer exec docker://ghcr.io/apptainer/lolcow echo \"I won't do what I was made for!\"\nI won't do what I was made for!\n[&lt;GUID&gt;@node1 [mars] ~]$\n</code></pre> <p>If you want to test and debug, you can run an interactive shell from within a container, using the <code>apptainer shell</code> command. To leave just use exit:</p> <pre><code>apptainer shell &lt;options&gt; &lt;image&gt;\n[&lt;GUID&gt;@node1 [mars] ~]$ apptainer shell docker://ghcr.io/apptainer/lolcow\nApptainer&gt; echo $SHELL\n/bin/bash\nApptainer&gt; echo \"Hello, world!\"\nHello, world!\nApptainer&gt; exit\n[&lt;GUID&gt;@node1 [mars] ~]$\n</code></pre> <p>All three of these commands have similar parameters. If you want to see all available options for the command run <code>man apptainer &lt;run/exec/shell&gt;</code> in your console. Below we explain some options we see as important:</p> Option Template Example Description <code>-B, --bind=[]</code> <code>-B &lt;src&gt;:&lt;dest&gt;</code> <code>-B ~/mydata:/datadir</code> Binds the local directory ~/mydata to /datadir within the container. <code>--nv</code> <code>--nv</code> <code>--nv</code> Enables Nvidia support. Use this option when working on GPU-servers to ensure you have the GPU available within your container."},{"location":"software-modules/environment-modules/","title":"Environment Modules","text":""},{"location":"tutorials/build-from-source/","title":"Building Software from Source","text":""},{"location":"tutorials/slurm-101/","title":"Slurm for Beginners","text":"<p>This small tutorial should give you a start to the world of Slurm! If you have any questions or issues, feel free to contact us! We help novice and expert users alike to get settled on MARS.</p> <p>If you are not familiar with the UNIX console, please do the UNIX Tutorial for Beginners first.</p> <p>For this whole tutorial you will find code snippets, these are displayed as interactions on a console. Lines with a preceding $ are the commands entered, ones without, are the output of these commands.</p> <pre><code>$ &lt;command-entered-by-user&gt;\n&lt;command-output&gt;\n</code></pre>"},{"location":"tutorials/slurm-101/#lesson-1-scheduler","title":"Lesson 1: Scheduler","text":"<p>The Scheduler, is the program running on the cluster, that schedules your jobs to be run on the resources available within the cluster. It ensures fair usage, and should under no circumstances be bypassed. In the case of the Central HPC, the scheduler is Slurm.</p> <p>Look at what resources you have available to you on the cluster, using the Slurm <code>sinfo</code> command. (The output displayed below is shortened)</p> <pre><code>$ sinfo -o \"%n %c %m %G\" | column -t\nHOSTNAMES  CPUS  MEMORY   GRES\nnode021     64    514048   (null)\nnode022     64    515072   (null)\nnode023     64    515072   (null)\n...\n</code></pre> <p>The nodes are grouped into partitions, this is a way to define what type of machine you want to work with. They are sometimes also referred to as queues. You can list all available partitions also using the <code>sinfo</code> command:</p> <pre><code>$ sinfo -o \"%P %D %N\" | column -t\nPARTITION  NODES  NODELIST\nnodes*     10     node[01-10]\ngpu        3      gpu[01-20]\n</code></pre>"},{"location":"tutorials/slurm-101/#lesson-2-interactive-job","title":"Lesson 2: Interactive Job","text":"<p>Interactive jobs are great to install software, prepare your environment or debug your script. It is also a good point to start to engage with HPC. But please, for any serious or larger work, use batch jobs. More on those in Lesson 3!</p> <p>You can start an interactive job using the <code>srun</code> command. You can tell, you are on a different server by the prompt, which should now feature the name of a compute node:</p> <pre><code>[login1]$ srun --pty bash\n[node01]$\n</code></pre> <p>Similar from how you switched from your PC via <code>ssh</code> to the login node, we now switched from the login node to the compute node using Slurm.</p> <p>In this session, we can start doing computational work. As an example, we will run some python3 code.</p> <ol> <li> <p>Load the python module <pre><code>$ module load apps/python3\n</code></pre></p> </li> <li> <p>Run python3 <pre><code>$ python3\n</code></pre></p> </li> <li> <p>Import python libraries <pre><code>&gt;&gt;&gt; import os\n</code></pre></p> </li> <li> <p>Get Slurm node name <pre><code>&gt;&gt;&gt; node = os.getenv(\"SLURMD_NODENAME\")\n</code></pre></p> </li> <li> <p>Open a new text file <pre><code>&gt;&gt;&gt; myfile = open(\"demofile.txt\", \"w\")\n</code></pre></p> </li> <li> <p>Write into the text file <pre><code>&gt;&gt;&gt; myfile.write(\"I am doing compute work on \" + node)\n</code></pre></p> </li> <li> <p>Exit python3 <pre><code>&gt;&gt;&gt; exit()\n</code></pre></p> </li> </ol> <p>Look at the file you just created using <code>cat</code>. You can see it used the name of the node we are working on in the output.</p> <pre><code>$ cat demofile.txt\nI am doing compute work on node01\n</code></pre> <p>This is our interactive job done, lets close our session with the exit command. This should bring us back to the login node, as indicated by the prompt again:</p> <pre><code>$ exit\nexit\n$\n</code></pre>"},{"location":"tutorials/slurm-101/#lesson-3-batch-job","title":"Lesson 3: Batch Job","text":"<p>If you are thinking the previous lesson was quite tedious and not very convenient, you will like batch job submissions.</p> <p>First, we have to save our python code into a file. For this create a file with the <code>.py</code> ending and add the lines we executed in python above in the interactive job. To make sure the output of our script is unique we will add the JobID as an identifier to the output file of the script.</p> <p>You can create and edit files using either <code>vi</code> or <code>nano</code>, whatever is more comfortable to you. Nano is the easier of the two, so if you are unfamiliar, we recommend using it. To exit and save in Nano do <code>ctrl+X \u2192 Y \u2192 Enter</code>.</p> <p><pre><code>$ nano myPythonCode.py\n</code></pre> <pre><code>import os\nnode = os.getenv(\"SLURMD_NODENAME\")\njobid = os.getenv(\"SLURM_JOBID\")\nmyfile = open(\"demofile-\" + jobid + \".txt\", \"w\")\nmyfile.write(\"I am doing compute work on \" + node)\nexit()\n</code></pre></p> <p>Now to schedule our job with Slurm , we need to create a job script. You can use a template from here Slurm Job Script Templates</p> <p>Under the \u201cLoading Modules\u201d subtitle add the required software, like in the interactive job example:</p> <pre><code>\u2026\n############# LOADING MODULES (optional) #############\nmodule load apps/python3\n\u2026\n</code></pre> <p>In the \u201cMy Code\u201d section run the script you saved earlier by providing it as a parameter to the python3 executable:</p> <pre><code>\u2026\n############# MY CODE #############\npython3 myPythonCode.py\n</code></pre> <p>Now you can submit your job using the <code>sbatch</code> utility and the path to the script. You will get a JobID in return, remember this to find your output later:</p> <pre><code>$ sbatch myPythonJob.sh\nSubmitted batch job &lt;JobID&gt;\n</code></pre> <p>After the job has finished, within your current working directory you should find an output file, which contains the <code>STDOUT</code> and an error file, which contains the <code>STDERR</code> of your job session. In addition, the file your python script created should also be there. Since your home storage is shared across all servers, you can see the output of your scripts in real time from the login node, even if the job ran on a compute node:</p> <pre><code>$ ls -l\nmyjob-&lt;JobID&gt;.out\nmyjob-&lt;JobID&gt;.err\ndemofile-&lt;JobID&gt;.txt\n</code></pre> <p>Check the output of these files. If all went as planned, the error file should be empty.</p> <p>You can run this job as many times as you want by just using the sbatch command. Like this you have access to the power of the HPC compute nodes, without ever having to log into one yourself.</p>"},{"location":"tutorials/slurm-101/#lesson-4-monitoring","title":"Lesson 4: Monitoring","text":"<p>You can see all running jobs on the system using the <code>squeue</code> command. If you only want to see your own jobs, you can specify your username with the <code>-u</code> parameter. If you only want to see a specific job, you can use the <code>-j</code> parameter followed by your JobID.</p> <pre><code>$ squeue\n$ squeue -u &lt;yourUsername&gt;\n$ squeue -j &lt;JobID&gt;\n</code></pre> <p>The output is as follows:</p> <ul> <li>JOBID: Unique identifier of the job. Counts up from 1 being the first job ever queued.</li> <li>PARTITION: Scheduler partition the job is queued into.</li> <li>NAME: Name of the job as defined by --job-name in your submission or your script name.</li> <li>USER: User who submitted the job to the scheduler.</li> <li>ST: Status of the job: R=Running, PD=Pending.</li> <li>TIME: Time the job has been running for.</li> <li>NODES: Number of nodes.</li> <li>NODELIST: List of names of allocated nodes.</li> </ul> <p>Slurm keeps a database with information of all jobs run using the system. To access this data, you can use the sacct command. Using the JobID you saved from your job, we can show a wide list of information for your job. Use the <code>-o</code> parameter followed by a list of Job-Accounting-Fields.</p> <p>A list of all available Job-Accounting-Fields can be found here: sacct manual</p> <p>Below is an example that gives you an overview of the requested resources for a job. You should see, these are the values you provided in the \u201cSLURM SETTINGS\u201d section of the script:</p> <pre><code>$ sacct -j &lt;JobID&gt; -o JobID,User,ReqCPUS,ReqMem,ReqNodes,TimeLimit -X\n</code></pre> <p>Now let\u2019s get some more information on how and where our job ran. In this output we see how long the job ran, and how it completed. An <code>ExitCode</code> of anything other than <code>0</code> usually means there was an error:</p> <pre><code>$ sacct -j &lt;JobID&gt; -o JobID,NodeList,Start,End,Elapsed,State,ExitCode -X\n</code></pre> <p>Say we want to check if our job ran efficiently, we could use the <code>seff</code> command. It uses data from the Slurm accounting database, to create information on how efficiently your job ran. We can use this information to make our jobs more efficient:</p> <pre><code>$ seff &lt;JobID&gt;\n</code></pre> <p>These efficiency values are only accurate, once your job has finished running. Accurate job scripts help the queuing system efficiently allocate shared resources. And therefore, your jobs should run quicker.</p> <p>For further reading on efficiency and general HPC-Etiquette, please read HPC Etiquette.</p>"},{"location":"tutorials/unix-101/","title":"UNIX for Beginners","text":"<p>This small tutorial should give you a start to the world of UNIX console! If you have any questions or issues, feel free to contact us! We help novice and expert users alike to get settled on MARS.</p> <p>For this whole tutorial you will find code snippets, these are displayed as interactions on a console. Lines with a preceding $ are the commands entered, ones without, are the output of these commands.</p> <pre><code>$ &lt;command-entered-by-user&gt;\n&lt;command-output&gt;\n</code></pre> <p>For all commands mentioned in the tutorial and most other UNIX commands, you can find a manual page with all the information about it. To access it just run <code>man</code> followed by the command. Here an example with <code>ls</code>:</p> <pre><code>$ man ls\n</code></pre> <p>Use the arrow keys to move up and down and to exit the manual just press q on your keyboard.</p>"},{"location":"tutorials/unix-101/#lesson-1-filesystem","title":"Lesson 1: Filesystem","text":"<p>The Filesystem is build hierarchically. When first login into the system with SSH, you\u2019ll get put into your home directory. On MARS this is <code>/users/&lt;GUID&gt;</code>, where <code>users</code> is the parent directory of your home. The prepending / (root) signifies that it is a full path rather than a relative path, which starts from where you are currently at in the hierarchy.</p> <p>As an example for this lesson, let\u2019s take this directory structure:</p> <pre><code>/\n\u2514 users\n  \u2514 999999x\n  \u2514 xx999x\n    \u2514 Documents\n      \u2514 myFile.txt\n    \u2514 Images\n\u2514 mnt\n  \u2514 data\n\u2514 tmp\n</code></pre> <p>You are user <code>xx999x</code> and therefore your journey starts in your home, marked yellow.</p> <p>The full path to the file <code>myFile.txt</code> is: <code>/users/xx999x/Documents/myFile.txt</code></p> <p>The relative path from your position after login in is: <code>Documents/myFile.txt</code></p>"},{"location":"tutorials/unix-101/#listing-files-and-directories","title":"Listing Files and Directories","text":"<p>To list the contents of a directory use the <code>ls</code> command. Using it without arguments will show you the contents of the directory you are currently in:</p> <pre><code>$ ls\nDocuments Images\n</code></pre> <p>Files with a prepending <code>.</code> are hidden files and will not be listed by default. To also show hidden files use the <code>-a</code> parameter:</p> <pre><code>$ ls -a\n. .. .bashrc Documents Images\n</code></pre> <p>You can supply a path to a directory you wish to list the contents of after the <code>ls</code> command:</p> <pre><code>$ ls Documents\nmyFile.txt\n</code></pre>"},{"location":"tutorials/unix-101/#changing-directories","title":"Changing Directories","text":"<p>To show the full path of the directory you are currently in you can use the <code>pwd</code> command, where pwd stands for print working directory:</p> <pre><code>$ pwd\n/users/xx999x\n</code></pre> <p>If you want to move around in the file system you can use the <code>cd</code> command, where cd stands for change directory. Using <code>cd</code> without arguments puts you back into your home directory.</p> <pre><code>$ pwd\n/users/xx999x/Documents\n$ cd\n$ pwd\n/users/xx999x\n</code></pre> <p>If you want to move into a specific directory you have to give the path to that directory as an argument:</p> <pre><code>$ pwd\n/users/xx999x\n$ cd Documents\n$ pwd\n/users/xx999x/Documents\n</code></pre> <p>Within every directory there are two directories, <code>.</code> for the current working directory and <code>..</code> for the parent directory. So if you want to go back, you can change into this directory:</p> <pre><code>$ pwd\n/users/xx999x/Documents\n$ cd ..\n$ pwd\n/users/xx999x\n</code></pre> <p>The symbol <code>~</code> (tilde) is used to refer to the home directory of a user. You can refer to this in your paths:</p> <pre><code>$ cd Images\n$ pwd\n/users/xx999x/Images\n$ cd ~/Documents\n$ pwd\n/users/xx999x/Documents\n</code></pre>"},{"location":"tutorials/unix-101/#lesson-2-managing-files","title":"Lesson 2: Managing Files","text":"<p>As an example for this lesson, let\u2019s take this structure:</p> <pre><code>/\n\u2514 users\n  \u2514 999999x\n  \u2514 xx999x\n    \u2514 Data\n      \u2514 oldFile.txt\n      \u2514 ImportantFiles\n        \u2514 importantFile.txt\n    \u2514 Documents\n      \u2514 myFile.txt\n    \u2514 Images\n\u2514 mnt\n  \u2514 data\n\u2514 tmp\n</code></pre> <p>You are user <code>xx999x</code> and therefore your journey starts in your home, marked yellow.</p>"},{"location":"tutorials/unix-101/#creating-directories","title":"Creating Directories","text":"<p>You can also expand the file structure by creating your own directories, as long as you have permission to do so, which within your home you should always have!</p> <p>For this use the <code>mkdir</code> command, where mkdir stands for make directory.</p> <pre><code>$ ls\nDocuments Images\n$ mkdir Data\n$ ls\nData Documents Images\n</code></pre>"},{"location":"tutorials/unix-101/#copying-files","title":"Copying Files","text":"<p>To copy a file you can use the cp command, where cp stands for copy. The first argument after the command is the source file, which you want to copy. The second argument is the destination and can be a directory or a path to a name, which the copied file will get renamed to:</p> <pre><code>$ cp Documents/myFile.txt Data/\n$ ls Data\nmyFile.txt\n$ cp Documents/myFile.txt Data/copyOfMyFile.txt\n$ ls Data\ncopyOfMyFile.txt myFile.txt\n</code></pre>"},{"location":"tutorials/unix-101/#moving-renaming-files","title":"Moving / Renaming Files","text":"<p>To move files around use the <code>mv</code> command, where mv stands for move. The first argument after the command is the source file, which you want to move. The second argument is the destination and can be a directory or a path to a name, which the moved file will get renamed to:</p> <pre><code>$ mv Documents/myFile.txt Data/\n$ ls Data\nmyFile.txt\n$ mv Data/myFile.txt Documents/myFileMoved.txt\n$ ls Documents\nmyFileMoved.txt\n</code></pre> <p>You can also use the <code>mv</code> utility to rename a file, by moving it into the same place but choosing a different destination name:</p> <pre><code>$ ls Documents\nmyFileMoved.txt\n$ mv Documents/myFileMoved.txt Documents/myFile.txt\n$ ls Documents\nmyFile.txt\n</code></pre>"},{"location":"tutorials/unix-101/#removing-items","title":"Removing Items","text":"<p>To remove files and directories you can use the <code>rm</code> command, where rm stands for remove. Be careful though, as this action can not be undone!</p> <p>For a single file you can just supply it as an argument:</p> <pre><code>$ ls Data\nImportantFiles oldFile.txt \n$ rm Data/oldFile.txt\n$ ls Data\nImportantFiles\n</code></pre> <p>If you want to remove a whole directory you have to use the <code>-r</code> parameter, to remove recursively.</p> <pre><code>$ ls Data\nImportantFiles\n$ rm -r /Data/ImportantFiles\n$ ls Data\n</code></pre>"},{"location":"tutorials/unix-101/#lesson-3-manipulating-text-files","title":"Lesson 3: Manipulating Text Files","text":"<p>As an example for this lesson, let\u2019s take this structure:</p> <pre><code>/\n\u2514 users\n  \u2514 999999x\n  \u2514 xx999x\n    \u2514 Data\n      \u2514 petData.csv\n    \u2514 Documents\n    \u2514 Images\n\u2514 mnt\n  \u2514 data\n\u2514 tmp\n</code></pre> <p>You are user <code>xx999x</code> and therefore your journey starts in your home, marked yellow.</p>"},{"location":"tutorials/unix-101/#creating-empty-files","title":"Creating empty files","text":"<p>To create empty files you can use the <code>touch</code> utility. <code>touch</code> is a software that changes a file\u2019s timestamp, but if the file supplied does not exist already, it will create it.</p> <pre><code>$ ls\nData Documents Images\n$ touch myFile.txt\n$ ls\nData Documents Images myFile.txt\n</code></pre>"},{"location":"tutorials/unix-101/#viewing-files","title":"Viewing files","text":"<p>You don\u2019t have to open a file in an editor to see its contents, you can use the <code>cat</code> utility, where cat stands for concatenate.</p> <pre><code>$ cat Data/petData.csv\nname,animal,age\nFluffy,dog,3\nMilky,cat,8\nRudolph,duck,2\nHank,cat,6\nPrincess,horse,12\nSnickers,dog,6\n</code></pre> <p>If you only want to see the beginning of a file, you can use the <code>head</code> command. With <code>-</code> and any number you can choose how many lines from the top you want to display:</p> <pre><code>$ head -2 Data/petData.csv\nname,animal,age\nFluffy,dog,3\n$ head -4 Data/petData.csv\nname,animal,age\nFluffy,dog,3\nMilky,cat,8\nRudolph,duck,2\n</code></pre> <p>If you only want to see the end of a file, you can use the <code>tail</code> command. With <code>-</code> and any number you can choose how many lines from the bottom you want to display:</p> <pre><code>$ tail -2 Data/petData.csv\nPrincess,horse,12\nSnickers,dog,6\n$ tail -4 Data/petData.csv\nRudolph,duck,2\nHank,cat,6\nPrincess,horse,12\nSnickers,dog,6\n</code></pre> <p>To remember these three commands, just think of the silhouette of a cat. If you want to see the whole cat, use <code>cat</code>. If you only want to see the top, use <code>head</code>. And if you only want to see the bottom use <code>tail</code>:</p>"},{"location":"tutorials/unix-101/#search-in-file","title":"Search in file","text":"<p>To search for a term or phrase within a file use the <code>grep</code> command. It returns you each line within a file, that matches the term you searched for and depending on your console settings also marks the word:</p> <pre><code>$ cat Data/petData.csv\nname,animal,age\nFluffy,dog,3\nMilky,cat,8\nRudolph,duck,2\nHank,cat,6\nPrincess,horse,12\nSnickers,dog,6\n$ grep \"dog\" Data/petData.csv\nFluffy,dog,3\nSnickers,dog,6\n</code></pre> <p>You can chain additional <code>grep</code> commands, to further search your results, just separate them with <code>|</code>.</p> <pre><code>$ grep \"dog\" Data/petData.csv\nFluffy,dog,3\nSnickers,dog,6\n$ grep \"dog\" Data/petData.csv | grep \"6\"\nSnickers,dog,6\n</code></pre>"},{"location":"tutorials/unix-101/#writing-output-to-file","title":"Writing output to file","text":"<p>You can save output of a command in UNIX to a file, instead of printing it to the console. This is helpful, when you run a command, that creates a lot of output you\u2019ll need to analyse later.</p> <p>The following example with <code>&gt;</code> writes the output (called <code>STDOUT</code>) of a command to a file and overwrites all existing content in that file. If the file does not exist already, it will be created.</p> <pre><code>$ ls\nData Documents Images\n$ ls &gt; myWorkDirectory.txt\n$ ls\nData Documents Images myWorkDirectory.txt\n$ cat myWorkDirectory.txt\nData Documents Images\n$ ls &gt; myWorkDirectory.txt\n$ cat myWorkDirectory.txt\nData Documents Images myWorkDirectory.txt\n</code></pre> <p>The example below with <code>&gt;&gt;</code> writes the output (called <code>STDOUT</code>) of a command to a file and appends it to the existing content in that file. If the file does not exist already, it will be created.</p> <pre><code>$ ls\nData Documents Images\n$ ls &gt;&gt; myWorkDirectory.txt\n$ ls\nData Documents Images myWorkDirectory.txt\n$ cat myWorkDirectory.txt\nData Documents Images\n$ ls &gt;&gt; myWorkDirectory.txt\n$ cat myWorkDirectory.txt\nData Documents Images\nData Documents Images myWorkDirectory.txt\n</code></pre>"},{"location":"tutorials/unix-101/#edit-file","title":"Edit file","text":"<p>To edit files, you can use a file editor. The most beginner friendly file editor is <code>nano</code>. If you run <code>nano</code> you will get put into an interactive program and lose access to the console:</p> <pre><code>$ nano Data/petData.csv\n</code></pre> <p>In your interactive editor, you can move around the file using the arrow keys on your keyboard and edit the file.</p> <p>After you are done making your changes, you can press <code>ctrl + X</code>, this will prompt the application to exit. Press <code>Y</code> and then <code>Enter</code> to save your changes or <code>N</code> to discard them.</p> <p>A more in depth explanation of <code>nano</code> can be found here.</p>"},{"location":"tutorials/unix-101/#lesson-4-filesystem-permissions","title":"Lesson 4: Filesystem Permissions","text":"<p>As an example for this lesson, let\u2019s take this structure:</p> <pre><code>/\n\u2514 users\n  \u2514 999999x\n  \u2514 xx999x\n    \u2514 Data\n      \u2514 myFile.txt\n\u2514 mnt\n  \u2514 data\n    \u2514 project9999\n      \u2514 ourResearchFile.txt\n\u2514 tmp\n</code></pre> <p>You are user <code>xx999x</code> and therefore your journey starts in your home, marked yellow.</p> <p>Every file and directory in UNIX has permissions. They are separated in three levels:</p> <ul> <li>The User (<code>u</code>) is usually the creator of the file or directory, only root can change this.</li> <li>The Group (<code>g</code>) is used to potentially share data with peers in a project.</li> <li>Other (<code>o</code>) is any other user on the system, this should be the layer that is most restricted.</li> </ul> <p>Permissions are denoted in a string of 10 characters. The first 1 is to identify the type of file. This can be either of those:</p> <ul> <li><code>d</code> for directories</li> <li><code>l</code> for symbolic links</li> <li><code>-</code> for an ordinary file</li> </ul> <p>The other 9 to determine the permission for every level described above. Each level has 3 characters dedicated:</p> <ul> <li><code>r</code> for read permission</li> <li><code>w</code> for write permission</li> <li><code>x</code> for execute permission (needed to run scripts or enter directories)</li> </ul> <p>If any of the characters are a <code>-</code> instead, that means that this permission is not granted on that level.</p> <p>Lets look at some examples of this: |Permission|Explaination| |----------|------------| |<code>drwx------</code>|This is a directory with full permissions only on the user level. This is the permission your home directory would have.| |<code>lrwxrwxrwx</code>|This is a symbolic link. Due to the nature of them always inheriting the permission of the resource they link to, they show as having full permission on all levels.| |<code>-rw-rw-r--</code>|This is a file with read + write permissions on the user and group level, and only read permissions on the other level. This is the default permission if you create a new file.|</p> <p>If you use the <code>-l</code> parameter for <code>ls</code>, you will get a long list output for a file or the components of a directory.</p> <pre><code>$ ls -l Data/myFile.txt\n-rw-rw-r-- 1 &lt;user&gt; &lt;group&gt; &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n</code></pre> <p>Here you can see who is the user / group level owner and the 10 character string in action!</p>"},{"location":"tutorials/unix-101/#changing-permissions","title":"Changing permissions","text":"<p>To change the permissions you can use the <code>chmod</code> command, where chmod stands for change mode. Define the level you want to modify (if nothing is provided, all levels get adjusted), the type of modification you are doing (<code>+</code>/<code>-</code>) and the permissions.</p> <pre><code>$ ls -l Data/myFile.txt\n-rw-rw-r-- 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n$ chmod o-r Data/myFile.txt\n$ ls -l Data/myFile.txt\n-rw-rw---- 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n$ chmod ug+rwx Data/myFile.txt\n$ ls -l Data/myFile.txt\n-rwxrwx--- 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n$ chmod -x Data/myFile.txt\n$ ls -l Data/myFile.txt\n-rw-rw---- 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n</code></pre> <p>As mentioned earlier, you can\u2019t change the owner of a file without <code>root</code> permissions, however you can change group. For this use the <code>chown</code> command, where chown stands for change owner. When changing the group permission you have to put a <code>:</code> before the owner group name:</p> <pre><code>$ ls -l /mnt/data/project9999/ourResearchFile.txt\n-rw-rw-r-- 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/ourResearchFile.txt\n$ chown :project9999 /mnt/data/project9999/ourResearchFile.txt\n$ ls -l /mnt/data/project9999/ourResearchFile.txt\n-rw-rw-r-- 1 xx999x project9999 &lt;size&gt; &lt;date-time&gt; Data/ourResearchFile.txt\n</code></pre> <p>This can be used to share files with your fellow project members.</p>"},{"location":"tutorials/unix-101/#special-permissions","title":"Special permissions","text":"<p>Special permissions are used to manipulate permission functionality to a file or directory and can be set on all three levels, each with different effects:</p>"},{"location":"tutorials/unix-101/#user-suid","title":"User (suid)","text":"<p>When set, the file is always run as the user owning the file, no matter who runs it. A well known example would be <code>/usr/bin/passwd</code>, which always runs as <code>root</code>.</p> <p>This configuration is shown as an <code>s</code> instead of an <code>x</code> on the user level:</p> <pre><code>$ ls -l /usr/bin/passwd \n-rwsr-xr-x. 1 root root ... /usr/bin/passwd\n</code></pre>"},{"location":"tutorials/unix-101/#group-sgid","title":"Group (sgid)","text":"<p>If set on a file, the file can be run as the group. The more common way to use it is on a directory, where if sgid is set, group ownership of files created is set the the directory group owner. You will find this configuration is standard in project directories.</p> <p>This configuration is shown as an <code>s</code> instead of an <code>x</code> on the group level:</p> <pre><code>$ ls -l /mnt/data/\ndrwxrws--- 4 root project9999 ... project9999\n</code></pre>"},{"location":"tutorials/unix-101/#other-sticky-bit","title":"Other (sticky bit)","text":"<p>This permission does not affect files. On directories however, it restricts file deletion and renaming. Only the owner user of a file can remove them within such a directory. This is most commonly used in <code>/tmp</code>, to allow everybody to write in it, but prevent others from deleting your work.</p> <p>This configuration is shown as a <code>t</code> instead of an <code>x</code> on the other level:</p> <pre><code>$ ls -la /\ndrwxrwxrwt. 1980 root root ... tmp\n</code></pre> <p>To set these special permissions, you can also use the <code>chmod</code> command. On either the user or group level set <code>s</code> for suid or sgid. For the sticky bit you need to use t:</p> <pre><code>$ ls -l Data/myFile.txt\n-rwxrwxrwx 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n$ chmod ug+s Data/myFile.txt\n$ ls -l Data/myFile.txt\n-rwsrwsrwx 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n$ chmod +t Data/myFile.txt\n$ ls -l Data/myFile.txt\n-rwsrwsrwt 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n</code></pre>"}]}