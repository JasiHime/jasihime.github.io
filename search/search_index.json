{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"This is my title","text":"<p>And this is some text</p> <ul> <li> <p>:material-clock-fast:{ .lg .middle } Set up in 5 minutes</p> <p>Install <code>mkdocs-material</code> with <code>pip</code> and get up and running in minutes</p> <p>:octicons-arrow-right-24: Getting started</p> </li> <li> <p>:fontawesome-brands-markdown:{ .lg .middle } It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site</p> <p>:octicons-arrow-right-24: Reference</p> </li> <li> <p>:material-format-font:{ .lg .middle } Made to measure</p> <p>Change the colors, fonts, language, icons, logo and more with a few lines</p> <p>:octicons-arrow-right-24: Customization</p> </li> <li> <p>:material-scale-balance:{ .lg .middle } Open Source, MIT</p> <p>Material for MkDocs is licensed under MIT and available on [GitHub]</p> <p>:octicons-arrow-right-24: License</p> </li> </ul>"},{"location":"Policies/usage-policy/","title":"Usage policy","text":"<p>To ensure the stability and usability of the platform for all users, there are certain rules and policies to abide by when using MARS. This is a living document, so changes can be made at any time. We will inform of major changes through our Team (Microsoft Teams). If you have any questions regarding the contents of this page, feel free to get in contact with us.</p> <ol> <li>Jobs must be run using the Slurm queueing system, interactive or batch jobs. Logging into any worker-nodes directly is forbidden.</li> <li>No compute intensive task is to be run on the login-node (mars-login.ice.gla.ac.uk). The login-node is to be used for minor code/script editing, data management, submitting and monitoring jobs. The MARS team reserves the right to kill processes, that constrain the login-node, without warning.</li> <li>Job submissions must specify the resources to the best estimate of the user, as to not waste cluster resources. This ensures the queueing system runs optimally and therefore the user\u2019s jobs are 5. starting as soon as possible. The MARS team reserves the right delete poorly specified jobs.</li> <li>Any data left on scratch spaces after the runtime of the job, may be deleted at any time and without notice.</li> <li>All storage on MARS is to be used only for active processing, and not for long term storage purposes. Please ensure, that all your scripts, datasets and results are saved to another system, as the MARS team offers limited data recovery services.</li> <li>Access to Project shares will only be granted with permission from the PI.</li> <li>The MARS team reserves the right to delete, kill or hold jobs or processes that have an adverse effect on the whole system\u2019s stability or health.</li> <li>The job submission system has the capability to send automated emails about the progress of jobs. Users should ensure that the number of requested emails is not excessive and refrain from sending emails to any email address outside the University of Glasgow domain. This means that affiliate/external user accounts should be using a UoG email address for job alerting.</li> <li>MARS is to be used for research purposes. Teaching and courses are not to be held using this cluster unless it was agreed upon prior with the MARS team.</li> <li>There is a maximum number of resources a single user can use on MARS. If their work needs more than the default, a project should be requested, as described here. The MARS team reserves the right to decline a project application, for reasons provided.</li> <li>To ensure the stability and security of MARS, maintenance is needed. This might result in access being restricted to the cluster or parts of it for certain periods of time. These maintenance windows will be announced via our Team (Microsoft Teams).</li> <li>The resources on MARS are free to use by members of the University of Glasgow. We reserve the right to change this and implement a charging model in the future.</li> </ol> <p>Please note that in addition to the MARS Usage Policy users must also comply with the general IT Policy of the University of Glasgow.</p>"},{"location":"system-information/compute-resources/","title":"Compute Resources","text":""},{"location":"system-information/scheduler/","title":"Scheduler (Slurm)","text":""},{"location":"system-information/scheduler/#default-and-maximum-values","title":"Default and Maximum Values","text":""},{"location":"system-information/scheduler/#partitions-queues","title":"Partitions / Queues","text":""},{"location":"user-docs/account-registration/","title":"Getting an Account","text":""},{"location":"user-docs/quickstart/","title":"Quickstart","text":""},{"location":"user-docs/quickstart/#access-the-cluster","title":"Access the Cluster","text":"<p>To access the system, you log into a login node. The login node is the central point of access to the cluster for all users. This server is not very powerful and should therefore not be used for computational work. Any computational work should go through a job allocation on the scheduler.</p>"},{"location":"user-docs/quickstart/#data-management","title":"Data Management","text":""},{"location":"user-docs/quickstart/#scheduler","title":"Scheduler","text":""},{"location":"user-docs/quickstart/#software","title":"Software","text":""},{"location":"user-docs/quickstart/#support","title":"Support","text":""},{"location":"user-docs/Tutorials/slurm-101/","title":"Slurm for Beginners","text":""},{"location":"user-docs/Tutorials/unix-101/","title":"UNIX for Beginners","text":""},{"location":"user-docs/guides/containers/","title":"Apptainer","text":""},{"location":"user-docs/guides/file-transfer/","title":"Transfer Data","text":""},{"location":"user-docs/guides/jupyter/","title":"Jupyter","text":""},{"location":"user-docs/guides/pbs-2-slurm/","title":"PBS to Slurm Cheat Sheet","text":""},{"location":"user-docs/guides/pbs-2-slurm/#user-commands","title":"User Commands","text":"Purpose PBS Slurm Submit a job via script <code>qsub myScript.sh</code> <code>sbatch myScript.sh</code> Cancel a job <code>qdel &lt;JobID&gt;</code> <code>scancel &lt;JobID&gt;</code> List all running jobs <code>qstat</code> <code>squeue</code> List all your running jobs <code>qstat -u &lt;GUID&gt;</code> <code>squeue -u &lt;GUID&gt;</code> Advanced job information <code>qstat -f &lt;JobID&gt;</code> <code>sacct -j &lt;JobID&gt;</code>"},{"location":"user-docs/guides/pbs-2-slurm/#environment-variables","title":"Environment Variables","text":"Purpose PBS Slurm Job ID <code>$PBS_JOBID</code> <code>$SLURM_JOBID</code> Submit directory <code>$PBS_O_WORKDIR</code> <code>$SLURM_SUBMIT_DIR</code> Allocated node list <code>$PBS_NODEFILE</code> <code>$SLURM_JOB_NODELIST</code> Current node name <code>-</code> <code>$SLURMD_NODENAME</code> Array job index <code>$PBS_ARRAY_INDEX</code> <code>$SLURM_ARRAY_TASK_ID</code> Number of CPUs <code>$PBS_NUM_PPN * $PBS_NUM_NODES</code> <code>$SLURM_NPROCS</code>"},{"location":"user-docs/guides/pbs-2-slurm/#job-parameters","title":"Job Parameters","text":"Purpose PBS Slurm Job name <code>#PBS -N myJob</code> <code>#SBATCH --job-name=myJob</code> Wallclock limit <code>#PBS -l walltime=hh:mm:ss</code> <code>#SBATCH --time=dd-hh:mm:ss</code> CPU Time <code>#PBS -l cput=hh:mm:ss</code> <code>-</code> Number of nodes and CPUs <code>#PBS -l nodes=2:ppn=8</code> <code>#SBATCH --nodes=2 --ntasks-per-node=1 --cpus-per-task=8</code> Memory <code>-</code> <code>#SBATCH --mem=8G</code> GPU <code>-</code> <code>#SBATCH --gres=gpu:1</code> Array <code>#PBS -t 1-100</code> <code>#SBATCH --array=1-100</code> Select queue / partition <code>#PBS -q &lt;QueueName&gt;</code> <code>#SBATCH --partition=&lt;PartitionName&gt;</code> Working directory <code>-</code> <code>#SBATCH --workdir=/path/to/file</code> STDOUT file <code>#PBS -o /path/to/file</code> <code>#SBATCH --output=/path/to/file</code> STDERR file <code>#PBS -e /path/to/file</code> <code>#SBATCH --error=/path/to/file</code> Email notifications <code>#PBS -m a\\|b\\|e</code> <code>#SBATCH --mail-type=NONE\\|BEGIN\\|END\\|FAIL\\|ALL</code> Email recipient <code>#PBS -M &lt;Email&gt;</code> <code>#SBATCH --mail-user=&lt;Email&gt;</code>"},{"location":"user-docs/guides/slurm/","title":"Slurm","text":""},{"location":"user-docs/software-modules/apptainer/","title":"Apptainer","text":""},{"location":"user-docs/software-modules/apptainer/#about","title":"About","text":"<p>Apptainer (formerly Singularity) simplifies the creation and execution of containers, ensuring software components are encapsulated for portability and reproducibility.</p> <p>Containers are images run on a server, using base operating system components from that server. It\u2019s similar to a virtual machine, where you can choose your OS and what packages to install within it, without compromising the host system.</p>"},{"location":"user-docs/software-modules/apptainer/#load-module","title":"Load Module","text":"CentralMARS <pre><code>module load apptainer\n</code></pre> <pre><code>module load apps/apptainer\n</code></pre>"},{"location":"user-docs/software-modules/apptainer/#usage","title":"Usage","text":"<p>For usage information run the following command after loading the module:</p> CentralMARS <pre><code>apptainer fake --help\n</code></pre> <pre><code>apptainer --help\n</code></pre> <p>Below we have some easy manuals to get a first feel for containers, though we would recommend going through the more in depth Quick Start guide Apptainer offers in their documentation. This is linked in the \u201cExternal Links\u201d below.</p>"},{"location":"user-docs/software-modules/apptainer/#run-containers","title":"Run containers","text":"<p>On MARS you can run images you made yourself, or run images provided by colleagues or software developers. Please make sure the images you are using are from trusted sources!</p> <p>You can run a container using the <code>apptainer run</code> command. The <code>&lt;image&gt;</code> part can either be the path to a <code>.sif</code> image you have saved locally or it can be a link to an image stored in a repository online.</p> <pre><code>apptainer run &lt;options&gt; &lt;image&gt;\n</code></pre> <p>As an example we will use the image hosted on the GitHub container registry by the Apptainer developers: docker://ghcr.io/apptainer/lolcow. All you need to do is provide the link and Apptainer will run the %runscript code of the image. In our example this will be a cow, telling you the current time and date:</p> <pre><code>[&lt;GUID&gt;@node1 [mars] ~]$ apptainer run docker://ghcr.io/apptainer/lolcow\n ______________________________\n&lt; Tue Feb 11 14:02:52 GMT 2025 &gt;\n ------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>If you want to run a command outside of the workflow defined in <code>%runscript</code>, you can use the <code>apptainer</code> exec command:</p> <pre><code>apptainer exec &lt;options&gt; &lt;image&gt; &lt;command&gt;\n</code></pre> <p>This will not run the code within <code>%runscript</code> and instead run the code you provide on the command line:</p> <pre><code>[&lt;GUID&gt;@node1 [mars] ~]$ apptainer exec docker://ghcr.io/apptainer/lolcow echo \"I won't do what I was made for!\"\nI won't do what I was made for!\n[&lt;GUID&gt;@node1 [mars] ~]$\n</code></pre> <p>If you want to test and debug, you can run an interactive shell from within a container, using the <code>apptainer shell</code> command. To leave just use exit:</p> <pre><code>apptainer shell &lt;options&gt; &lt;image&gt;\n[&lt;GUID&gt;@node1 [mars] ~]$ apptainer shell docker://ghcr.io/apptainer/lolcow\nApptainer&gt; echo $SHELL\n/bin/bash\nApptainer&gt; echo \"Hello, world!\"\nHello, world!\nApptainer&gt; exit\n[&lt;GUID&gt;@node1 [mars] ~]$\n</code></pre> <p>All three of these commands have similar parameters. If you want to see all available options for the command run <code>man apptainer &lt;run/exec/shell&gt;</code> in your console. Below we explain some options we see as important:</p> Option Template Example Description <code>-B, --bind=[]</code> <code>-B &lt;src&gt;:&lt;dest&gt;</code> <code>-B ~/mydata:/datadir</code> Binds the local directory ~/mydata to /datadir within the container. <code>--nv</code> <code>--nv</code> <code>--nv</code> Enables Nvidia support. Use this option when working on GPU-servers to ensure you have the GPU available within your container."}]}